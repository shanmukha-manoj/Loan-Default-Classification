# -*- coding: utf-8 -*-
"""MLproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Svoj6JK47Nu2LuZPfTftp3BqbkS7Dogv
"""

#importing the libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

#importing the training set
train_data = pd.read_csv('credit_train.csv')
train_data.head(2)

#shape of training dataset
train_data.shape

"""Note: The training dataset google sheet is having only 100000 samples,   all the last 514 rows are filled with NULL values as default,    so to reduce the error it's better to remove the last 514 NULL rows"""

#removing the last 514 rows from the training dataset
lister = [x for x in range(100000,100514)]
train_data.drop(lister,inplace = True)
print(train_data.shape)

# training set data analysis
train_data.info()

#checking the NULL values in each column
train_data.isnull().sum()

#importing the test data
test_data = pd.read_csv('credit_test.csv')
test_data.head(2)

# checking the shape of test data
test_data.shape

"""Note: The test dataset google sheet is having only 10,000 samples, all the last 353 rows are filled with NULL values as default, so to reduce the error it's better to remove the last 352 NULL rows."""

#removing the last 352 rows from the test dataset
lister = [x for x in range(10000,10353)]
test_data.drop(lister,inplace = True)
print(test_data.shape)

#test set data analysis
test_data.info()

#checking the NULL values in each column
test_data.isnull().sum()

"""DIMENTIONALITY REDUCTION"""

# Reducing the number of columns by observing the Corelation matrix
train_data.corr()

#Now let's draw a heatmap to have an idea about the correlation
import seaborn as sns
plt.figure(figsize=(13,10))
plt.title("Data Correlation")
sns.heatmap(train_data.corr())

# making a analysis between Loan Status and Term
pd.crosstab(index=train_data['Loan Status'],columns=train_data['Term']).plot(kind='bar')
plt.ylabel('Count')

"""Observation from the above graph is : Most of the customers who took the short term loan paid completly."""

# making a analysis between Loan Status and Purpose
pd.crosstab(index=train_data['Loan Status'],columns=train_data['Purpose']).plot(kind='bar')
plt.ylabel('Count')
plt.figure(figsize=(40,20))

"""most of the customers who took loan for small business paid completly."""

# making a analysis between Loan Status and Home Ownership
pd.crosstab(index=train_data['Loan Status'],columns=train_data['Home Ownership']).plot(kind='bar')
plt.ylabel('Count')
plt.figure(figsize=(40,20))

# removing some columns from training and test datsets 
train_data = train_data.drop(columns = ['Loan ID','Customer ID','Years in current job'])
test_data = test_data.drop(columns = ['Loan ID','Customer ID','Years in current job'])

#shape of the datasets after deleting the columns
print(train_data.shape)
print(test_data.shape)

"""Note : In the datasets some columns are Categorical data so those columns must be converted into numbered data.........this can be done just by mapping those strings to numbers.

CONVERTING THE CATEGORICAL DATA INTO NORMAL NUMBERED DATA IN BOTH TRAIN AND TEST DATASETS
"""

#mapping the categorical data in training dataset 
train_data['Purpose'] = train_data['Purpose'].map({'Home Improvements':0 , 'Debt Consolidation':1 , 'Buy House':2,'other':3,'Business Loan':4, 'Buy a Car':5, 'major_purchase':6, 'Take a Trip':7,
       'Other':8, 'small_business':9, 'Medical Bills':10, 'wedding':11, 'vacation':12,
       'Educational Expenses':13, 'moving':14, 'renewable_energy':15})

train_data['Home Ownership'] = train_data['Home Ownership'].map({'Home Mortgage':0, 'Own Home':1, 'Rent':2, 'HaveMortgage':3})

train_data['Term'] = train_data['Term'].map({'Short Term':0,'Long Term':1})

train_data['Loan Status'] = train_data['Loan Status'].map({'Fully Paid':0 , 'Charged Off':1}) 

#mapping the categorical data in test dataset

test_data['Purpose'] = test_data['Purpose'].map({'Home Improvements':0 , 'Debt Consolidation':1 , 'Buy House':2,'other':3,'Business Loan':4, 'Buy a Car':5, 'major_purchase':6, 'Take a Trip':7,
       'Other':8, 'small_business':9, 'Medical Bills':10, 'wedding':11, 'vacation':12,
       'Educational Expenses':13, 'moving':14, 'renewable_energy':15})

test_data['Home Ownership'] = test_data['Home Ownership'].map({'Home Mortgage':0, 'Own Home':1, 'Rent':2, 'HaveMortgage':3})

test_data['Term'] = test_data['Term'].map({'Short Term':0,'Long Term':1})

"""Note : NULL values are present in both training and test datasets.....so filling of those NULL values should be done before training with the algorithms


Note: NULL values of categorical data columns should be filled with MODE and the NULL values in numbered data columns should be filled with MEAN.

FILLING THE NULL VALUES IN TRAINING DATASET
"""

# filling the NULL values in Numbered data columns using mean function
train_data['Credit Score'] = train_data['Credit Score'].fillna(train_data['Credit Score'].mean())
train_data['Annual Income'] = train_data['Annual Income'].fillna(train_data['Annual Income'].mean())
train_data['Months since last delinquent'] = train_data['Months since last delinquent'].fillna(train_data['Months since last delinquent'].mean())
train_data['Maximum Open Credit'] = train_data['Maximum Open Credit'].fillna(train_data['Maximum Open Credit'].mean())
train_data['Bankruptcies'] = train_data['Bankruptcies'].fillna(train_data['Bankruptcies'].mean())
train_data['Tax Liens'] = train_data['Tax Liens'].fillna(train_data['Tax Liens'].mean())

train_data.isnull().sum()

"""FILLING THE NULL VALUES IN TEST DATASET"""

# filling the NULL values in Numbered data columns using mean function
test_data['Credit Score'] = test_data['Credit Score'].fillna(test_data['Credit Score'].mean())
test_data['Annual Income'] = test_data['Annual Income'].fillna(test_data['Annual Income'].mean())
test_data['Months since last delinquent'] = test_data['Months since last delinquent'].fillna(test_data['Months since last delinquent'].mean())
test_data['Bankruptcies'] = test_data['Bankruptcies'].fillna(test_data['Bankruptcies'].mean())
test_data['Tax Liens'] = test_data['Tax Liens'].fillna(test_data['Tax Liens'].mean())

test_data.isnull().sum()

"""Now after using 'fillna' function both Training and test datasets are having zero NULL values.

DIVIDING THE COLUMNS INTO FEATURES AND LABEL
"""

#Splitting the columns
train_features = train_data.drop(columns = ['Loan Status'])
train_label = train_data['Loan Status']

"""NORMALISE THE TRAINING AND TEST DATA FOR BEST ACCURACY"""

# normailising the training dataset
from sklearn import preprocessing
x_norm = preprocessing.normalize(train_features.T)
train_features = x_norm.T
print(train_features)

# normailising the test dataset
from sklearn import preprocessing
y_norm = preprocessing.normalize(test_data.T)
test_data = y_norm.T
print(test_data)

"""Note : In the test dataset the required label is not given....so to get an idea about train and test accuracies the training dataset is splitted.

SPLITTING THE TRAINING DATASET
"""

#splitting the training dataset 
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(train_features,train_label,test_size=0.30)

# shapes of datasets after splitting
print(train_features.shape)
print(train_label.shape)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""K NEIGHBOR ALGORITHM"""

# Training the model using KNeighbor algorithm by varying no.of Neighbors from 2 to 9
from sklearn.neighbors import KNeighborsClassifier
train_acc_array=[]
test_acc_array=[]
for i in range(2,9):
  kmodel = KNeighborsClassifier(n_neighbors = i)
  kmodel.fit(x_train,y_train)
  pre_train = kmodel.predict(x_train)
  train_acc_array.append((pre_train == y_train).sum()/len(y_train))
  pre_test = kmodel.predict(x_test)
  test_acc_array.append((pre_test == y_test).sum()/len(y_test))


# plotting a graph of Training and test accuracies varying the number of neighbours

plt.plot(np.arange(2,9) , train_acc_array , label= 'Train_Acc')
plt.plot(np.arange(2,9) , test_acc_array , label = 'Test_Acc')
plt.xlabel('No. of neighbours')
plt.ylabel('Accuracy percent')
plt.legend()

# From the above graph we came to know that the best train and test accuracies are at n=7
kmodel = KNeighborsClassifier(n_neighbors= 6)

# fit the model
kmodel.fit(x_train, y_train)

# Calculate and print the scores
print("Training Accuracy is", kmodel.score(x_train, y_train))
print("Test Accuracy is", kmodel.score(x_test, y_test))

# storing these accuracies in some variables
kmodel_tr_acc=kmodel.score(x_train,y_train)
kmodel_te_acc=kmodel.score(x_test,y_test)

"""After training my model with KNeighbor algorithm the final training and test accuracies are 83.30% and 80.86%

LOGISTIC REGRESSION ALGORITHM
"""

# importing the logistic regression algorithm
#importing the algorithm
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression()

# fitting the training data
LR.fit(x_train,y_train)

# printing the training accuracy and test accuracy
print('Training accuracy is',LR.score(x_train,y_train))
print('Testing accuracy is',LR.score(x_test,y_test))


# storing these accuracies in some variables
LR_tr_acc=LR.score(x_train,y_train)
LR_te_acc=LR.score(x_test,y_test)

"""After training my model with Logistic Regression algorithm the final training and test accuracies are 77.4% and 77.27%

Note : The difference between these accuracies is very less which is a good sign to go further.

SVM ALGORITHM
"""

#importing the SVM algorithm 
from sklearn.svm import SVC
SVM = SVC()

#fitting the data
SVM.fit(x_train,y_train)

#printing the accuracies.
print(SVM.score(x_train,y_train))
print(SVM.score(x_test,y_test))

#storing the accuracies in some variables

SVM_tr_acc=SVM.score(x_train,y_train)
SVM_te_acc=SVM.score(x_test,y_test)

"""After training my model with SVM algorithm the final training and test accuracies are 81.99% and 81.79%

RANDOM FOREST ALGORITHM
"""

#importing RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=0)

# fitting the data
RFC.fit(x_train, y_train)

# print the accuracies
print('Training Accuracy is',RFC.score(x_train, y_train))
print('Test Accuracy is',RFC.score(x_test, y_test))


#storing the accuracies in some variables

RFC_tr_acc=RFC.score(x_train,y_train)
RFC_te_acc=RFC.score(x_test,y_test)

"""After training my model with Random Forest algorithm the final training and test accuracies are 81.95% and 81.80%

STOCHASTIC GRADIENT DESCENT (SGD)
"""

#importing the algorithm
from sklearn.linear_model import SGDClassifier
SGD = SGDClassifier(loss="hinge", penalty="l1",alpha = 900,max_iter=375)

#fitting the data
SGD.fit(x_train,y_train)

# print the accuracies
print('Training Accuracy is',SGD.score(x_train, y_train))
print('Test Accuracy is',SGD.score(x_test, y_test))

#storing the accuracies in some variables

SGD_tr_acc=SGD.score(x_train,y_train)
SGD_te_acc=SGD.score(x_test,y_test)

"""After training my model with SGD algorithm the final training and test accuracies are 77.4% and 77.27%

COMPARISION BETWEEN THESE ALGORITHMS
"""

#comparision of algorithm by plotting a bar chart

model_names = ['KNeighbor' , 'Logistic Regression' , 'SVM' , 'Random Forest' , 'SGD']
tr_acc_list = [kmodel_tr_acc , LR_tr_acc , SVM_tr_acc , RFC_tr_acc , SGD_tr_acc]
te_acc_list = [kmodel_te_acc , LR_te_acc , SVM_te_acc , RFC_te_acc , SGD_te_acc]

#plotting a bar graph
plt.figure(figsize=(10,7))
ind = np.arange(len(model_names))
width = 0.25
plt.bar(ind,tr_acc_list, width, label='Train_acc')
plt.bar(ind+width,te_acc_list, width, label='Test_acc')
plt.ylabel('Accuracy Percent')
plt.title('Algorithm vs accuracy')
plt.xticks(ind + width / 2, model_names)
plt.legend(loc = 'upper right')
plt.show()

plt.figure(figsize=(10,7))
plt.plot(model_names,tr_acc_list,label='Train_accuracy')
plt.plot(model_names,te_acc_list,label='Test_accuracy')
plt.legend(loc = 'upper right')
plt.ylabel('Accuracy Percentage')
plt.show()

"""CONCLUSION :


KNeighbor: Training Accuracy - 0.8330   ,     Test Accuracy - 0.8086

Logistic Regression: Training Acc - 0.774     ,     Testing Acc - 0.7727

SVM: Training acc -  0.8199    ,      Testing acc  - 0.8179

RandomForest Classifier: Training Accuracy  -  0.8195, Test Accuracy  - 0.8180

SGD: Training Accuracy   -  0.774, Test Accuracy  -  0.7727



WE CAN CONCLUDE THAT IF WE TRAIN OUR MODEL WITH KNEIGHBOR WE GET THE BEST ACCURACY

PREDICTING THE ORIGINAL TEST DATASET WITH KNEIGHBOR
"""

test_output = kmodel.predict(test_data)
print(test_output)

test_output.shape

count=0
for i in range(10000):
  if (test_output[i] == 0):
    count =count + 1
print(count)

"""AS PER KNEIGHBOR ALGORITHM PREDICTION......OUT OF 10000 CUSTOMERS IN TEST DATASET    9342    CUSTOMERS WILL PAY COMPLETLY AND THE REMAINING    658     CUSTOMERS WILL NOT PAY THE LOAN BACK."""